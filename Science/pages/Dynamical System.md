- # Definition
	- A system that changes its state in time
	- In math lingo, a dynamical system is a set of  *differential equations* which are usually coupled.
		- Example:
			- The system state: $(x_1, x_2, x_3, \dots, x_n)$
			- The Dynamical System, ($f$ is the function of all the state variables )
				- $$
				  \frac{d x_1}{dt} = f(x_1, \dots, x_n) \\
				  \frac{d x_2}{dt} = f(x_1, \dots, x_n) \\
				  \vdots \\
				  \frac{d x_n}{dt} = f(x_1, \dots, x_n)
				  $$
				-
- You can draw a picture of state and the derivtives:
	- The values of derivtives show the tendency of the state.
- # Fixed Point
  id:: 6224c16a-48ac-451a-9766-09930b7fbf34
	- The location where the time derivtive equals to 0
	- Classess:
		- Stable
			- You nudge it a liitle bit, it will stay where it is.
		- Unstable
- # 1-D
	- An example: